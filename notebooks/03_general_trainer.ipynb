{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "prompts = {\n",
    "    'spam': 'text message with advertisement or offer',\n",
    "    'non_spam':'text message from a friend or family says',\n",
    "    'real_news':'recently published political news title',\n",
    "    'fake_news':'fake political news title',\n",
    "    'happy_tweet':'tweet as a happy person',\n",
    "    'sad_tweet': 'tweet as a sad person'\n",
    "}\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    for p in prompts:\n",
    "        text = text.replace(prompts[p], \"\")\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZERO_SHOT = \"zs\"\n",
    "FEW_SHOT = \"fs\"\n",
    "PARAPHRASE = \"pp\"\n",
    "\n",
    "GPT2 = \"gpt2\"\n",
    "FLAN = \"flan\"\n",
    "LLAMA = \"llama\"\n",
    "\n",
    "class Utils:\n",
    "    @staticmethod    \n",
    "    def load_data(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['text']=df['text'].apply(clean_text)\n",
    "        df = df.dropna()        \n",
    "        return df['text'].tolist(), df['y'].tolist()\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_data_loader(texts, labels, tokenizer, max_len, batch_size):\n",
    "        dataset = TextDataset(texts, labels, tokenizer, max_len)\n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_mini_train_data(task):\n",
    "        return \"../data/orig/processed/train/\"+task+\"-data-mini.csv\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_syn_train_data(llm, synthetic_data_type, task):\n",
    "        return \"../data/syn/\"+llm+\"/\"+synthetic_data_type+\"/auto-\"+task+\"-data.csv\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_mini_test_data(task):\n",
    "        return \"../data/orig/processed/test/\"+task+\"-data.csv\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_dataset(llm, synthetic_data_type, task):\n",
    "        if synthetic_data_type==ZERO_SHOT or synthetic_data_type==PARAPHRASE or synthetic_data_type==FEW_SHOT:\n",
    "            return Utils.get_syn_train_data(llm, synthetic_data_type, task)\n",
    "        return Utils.get_mini_train_data(task)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_model_path(llm, synthetic_data_type, task):\n",
    "        return \"../models/\" + llm + \"/\" + task + \"-\" + synthetic_data_type + \"-\" + llm + \"-model.pth\"\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "class TextClassifier(torch.nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.bert = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        return self.bert(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, device):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for data in tqdm(data_loader):\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        labels = data['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        \n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n",
    "\n",
    "def eval_model(model, data_loader, device):\n",
    "    model = model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            input_ids = data['input_ids'].to(device)\n",
    "            attention_mask = data['attention_mask'].to(device)\n",
    "            labels = data['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            \n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return accuracy_score(true_labels, predictions), classification_report(true_labels, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    MAX_LEN = 32  # Maximum sequence length for BERT\n",
    "    BATCH_SIZE = 16\n",
    "    EPOCHS = 5\n",
    "    LEARNING_RATE = 2e-5\n",
    "    MODEL_NAME = 'bert-base-uncased'  # BERT model type\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(Config.MODEL_NAME)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_model(file_path):\n",
    "    # Load data\n",
    "    texts, labels = Utils.load_data(file_path)\n",
    "    num_classes = len(set(labels))  # Number of unique labels\n",
    "\n",
    "    print(\"Tokenizing and Splitting\")\n",
    "\n",
    "    # Split into train and validation sets\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "    train_data_loader = Utils.create_data_loader(train_texts, train_labels, tokenizer, Config.MAX_LEN, Config.BATCH_SIZE)\n",
    "    val_data_loader = Utils.create_data_loader(val_texts, val_labels, tokenizer, Config.MAX_LEN, Config.BATCH_SIZE)\n",
    "\n",
    "    # Initialize Model\n",
    "    model = TextClassifier(Config.MODEL_NAME, num_labels=num_classes)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=Config.LEARNING_RATE)\n",
    "\n",
    "    print(\"Training\")\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(Config.EPOCHS):\n",
    "        print(f\"Epoch {epoch + 1}/{Config.EPOCHS}\")\n",
    "        train_acc, train_loss = train_epoch(model, train_data_loader, torch.nn.CrossEntropyLoss(), optimizer, device)\n",
    "        print(f\"Train loss: {train_loss}, Train accuracy: {train_acc}\")\n",
    "\n",
    "        val_acc, val_report = eval_model(model, val_data_loader, device)\n",
    "        print(f\"Validation accuracy: {val_acc}\")\n",
    "        if(epoch==Config.EPOCHS-1):\n",
    "            print(f\"Classification report:\\n{val_report}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'news': {'MAX_LEN': 32,\n",
       "  'BATCH_SIZE': 10,\n",
       "  'EPOCHS': 3,\n",
       "  'train_data': '../data/syn/llama/zs/auto-news-data.csv',\n",
       "  'test_data': '../data/orig/processed/test/news-data.csv'},\n",
       " 'spam': {'MAX_LEN': 48,\n",
       "  'BATCH_SIZE': 50,\n",
       "  'EPOCHS': 3,\n",
       "  'train_data': '../data/syn/llama/zs/auto-spam-data.csv',\n",
       "  'test_data': '../data/orig/processed/test/spam-data.csv'},\n",
       " 'sentiment': {'MAX_LEN': 48,\n",
       "  'BATCH_SIZE': 40,\n",
       "  'EPOCHS': 4,\n",
       "  'train_data': '../data/syn/llama/zs/auto-sentiment-data.csv',\n",
       "  'test_data': '../data/orig/processed/test/sentiment-data.csv'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_TYPE = ZERO_SHOT # zs, fs, pp, n\n",
    "LLM = LLAMA\n",
    "tasks = {\n",
    "    'news':{\n",
    "        'MAX_LEN':32,\n",
    "        'BATCH_SIZE':10,\n",
    "        'EPOCHS':3,\n",
    "    },\n",
    "    'spam':{\n",
    "        'MAX_LEN':48,\n",
    "        'BATCH_SIZE':50,\n",
    "        'EPOCHS':3,\n",
    "    },\n",
    "    'sentiment':{\n",
    "        'MAX_LEN':48,\n",
    "        'BATCH_SIZE':40,\n",
    "        'EPOCHS':4,\n",
    "    }\n",
    "}\n",
    "for task in tasks:\n",
    "    tasks[task]['train_data'] = Utils.get_dataset(LLM, DATASET_TYPE,task)\n",
    "    tasks[task]['test_data'] = Utils.get_mini_test_data(task)\n",
    "df_out = pd.DataFrame(columns=['Task','Accuracy'])\n",
    "tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../models/llama/spam-zs-llama-model.pth'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Utils.get_model_path(LLM, DATASET_TYPE, \"spam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training:  news\n",
      "Train Data on:  ../data/syn/llama/zs/auto-news-data.csv\n",
      "Tokenizing and Splitting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\sreya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:51<00:00,  4.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6051501658829775, Train accuracy: 0.7407407407407407\n",
      "Validation accuracy: 1.0\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:52<00:00,  4.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2592696493322199, Train accuracy: 0.9907407407407407\n",
      "Validation accuracy: 1.0\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:49<00:00,  4.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.056214520877057854, Train accuracy: 1.0\n",
      "Validation accuracy: 1.0\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        15\n",
      "           1       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        28\n",
      "   macro avg       1.00      1.00      1.00        28\n",
      "weighted avg       1.00      1.00      1.00        28\n",
      "\n",
      "Ending training:  news\n",
      "Starting TEST\n",
      "Tokenize\n",
      "Evaluate\n",
      "Test Accuracy:  0.5217391304347826\n",
      "Starting training:  spam\n",
      "Train Data on:  ../data/syn/llama/zs/auto-spam-data.csv\n",
      "Tokenizing and Splitting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\sreya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [08:40<00:00, 26.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.29955361485481263, Train accuracy: 0.9293873312564901\n",
      "Validation accuracy: 1.0\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [08:22<00:00, 25.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.032298611896112564, Train accuracy: 1.0\n",
      "Validation accuracy: 1.0\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [05:18<04:20, 28.96s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training: \u001b[39m\u001b[38;5;124m\"\u001b[39m, task)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Data on: \u001b[39m\u001b[38;5;124m\"\u001b[39m,tasks[task][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_data\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 14\u001b[0m models[task] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(models[task]\u001b[38;5;241m.\u001b[39mstate_dict(), Utils\u001b[38;5;241m.\u001b[39mget_model_path(LLM, DATASET_TYPE, task))\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnding training: \u001b[39m\u001b[38;5;124m\"\u001b[39m, task)\n",
      "Cell \u001b[1;32mIn[3], line 35\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(Config\u001b[38;5;241m.\u001b[39mEPOCHS):\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mConfig\u001b[38;5;241m.\u001b[39mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 35\u001b[0m     train_acc, train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m     val_acc, val_report \u001b[38;5;241m=\u001b[39m eval_model(model, val_data_loader, device)\n",
      "Cell \u001b[1;32mIn[2], line 98\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, data_loader, loss_fn, optimizer, device)\u001b[0m\n\u001b[0;32m     95\u001b[0m correct_predictions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(preds \u001b[38;5;241m==\u001b[39m labels)\n\u001b[0;32m     96\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m---> 98\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    100\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for task in tasks:\n",
    "    if(task in ['sentiment']):\n",
    "        continue\n",
    "    # if(task in ['spam','news']):\n",
    "    #     continue\n",
    "    class Config:\n",
    "        MAX_LEN = tasks[task]['MAX_LEN']  # Maximum sequence length for BERT\n",
    "        BATCH_SIZE = tasks[task]['BATCH_SIZE']\n",
    "        EPOCHS = tasks[task]['EPOCHS']\n",
    "        LEARNING_RATE = 2e-5\n",
    "        MODEL_NAME = 'bert-base-uncased'\n",
    "    print(\"Starting training: \", task)\n",
    "    print(\"Train Data on: \",tasks[task]['train_data'])\n",
    "    models[task] = train_model(tasks[task]['train_data'])\n",
    "    torch.save(models[task].state_dict(), Utils.get_model_path(LLM, DATASET_TYPE, task))\n",
    "    print(\"Ending training: \", task)\n",
    "    print(\"Starting TEST\")\n",
    "    test_texts, test_labels = Utils.load_data(tasks[task]['test_data'])\n",
    "    print(\"Tokenize\")\n",
    "    test_data_loader = Utils.create_data_loader(test_texts, test_labels, tokenizer, Config.MAX_LEN, Config.BATCH_SIZE)\n",
    "    print(\"Evaluate\")\n",
    "    tasks[task]['predictions'] = eval_model(models[task], test_data_loader, device)\n",
    "    print(\"Test Accuracy: \", tasks[task]['predictions'][0])\n",
    "    df_out.loc[len(df_out)] = [task, tasks[task]['predictions'][0]]\n",
    "\n",
    "\n",
    "df_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
