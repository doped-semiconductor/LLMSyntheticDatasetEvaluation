{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model: GPT-2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name, pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utils**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_rows(task):\n",
    "    file = \"../data/orig/processed/train/\" + task + \"-data-mini.csv\"\n",
    "    df = pd.read_csv(file)\n",
    "    return int(df.shape[0]/2)\n",
    "\n",
    "def load_data(task):\n",
    "    if(task == \"news\"):\n",
    "        df_1 = pd.read_csv(\"..\\\\data\\\\orig\\\\main\\\\news\\\\BuzzFeed_fake_news_content.csv\")\n",
    "        df_2 = pd.read_csv(\"..\\\\data\\\\orig\\\\main\\\\news\\\\BuzzFeed_real_news_content.csv\")\n",
    "\n",
    "        df_1['text'] = df_1['title'] + ' ' + df_1['text']\n",
    "        df_2['text'] = df_2['title'] + ' ' + df_2['text']\n",
    "\n",
    "        df_1 = df_1[['text']]\n",
    "        df_2 = df_2[['text']]\n",
    "\n",
    "        df_1['y'] = 1\n",
    "        df_2['y'] = 0\n",
    "\n",
    "        df1_train, df1_test = train_test_split(df_1, random_state=42)\n",
    "        df2_train, df2_test = train_test_split(df_2, random_state=36)\n",
    "\n",
    "        df1 = pd.concat([df1_train, df2_train], ignore_index=True)\n",
    "        df1 = df1.sample(frac=1).reset_index(drop=True)\n",
    "        return df1\n",
    "    \n",
    "    if(task == \"spam\"):\n",
    "        df = pd.read_csv(\"../data/orig/main/spam/data.csv\", encoding='ISO-8859-1')\n",
    "        df = df[[\"v1\", \"v2\"]]\n",
    "        df[\"v1\"] = df[\"v1\"].apply(lambda x: 1 if x==\"spam\" else 0)\n",
    "        df.rename(columns={\"v1\":\"y\",\"v2\":\"text\"}, inplace=True)\n",
    "        df = df.sample(frac=1).reset_index(drop=True)\n",
    "        df1, df2 = train_test_split(df, test_size=0.2, random_state=65)\n",
    "        df1.dropna(inplace=True)\n",
    "        df1 = pd.concat([df1[df1['y']==1], df1[df1['y']==0].sample(n=602)], ignore_index=True)\n",
    "        return df1\n",
    "    df = pd.read_csv(\"../data/orig/main/sentiment/data.csv\", encoding='latin-1', header=None)\n",
    "    df = df[[0,5]]\n",
    "    df.rename(columns={0:'y',5:'text'}, inplace=True)\n",
    "    df['y'] = df['y'].apply(lambda x: 1 if x==4 else 0)\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    df1, df2  = train_test_split(df, random_state=46, test_size=0.3)\n",
    "    df1.dropna(inplace=True)\n",
    "    n = 2500\n",
    "    df1 = pd.concat([df1[df1['y']==1].sample(n=n), df1[df1['y']==0].sample(n=n)], ignore_index=True)\n",
    "    return df1\n",
    "\n",
    "zsl_tasks = [\n",
    "    {\n",
    "        'name':'news',\n",
    "        'rows': count_rows('news'),\n",
    "        'print_count':10\n",
    "    },\n",
    "    {\n",
    "        'name':'spam',\n",
    "        'rows': count_rows('spam'),\n",
    "        'print_count':100\n",
    "    },    \n",
    "    {\n",
    "        'name':'sentiment',\n",
    "        'rows': count_rows('sentiment'),\n",
    "        'print_count':250\n",
    "    }\n",
    "]\n",
    "\n",
    "def generate_text_from_prompt(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=49)\n",
    "    \n",
    "    outputs = model.generate(**inputs, max_length=50, num_return_sequences=1, do_sample=True, top_k=0)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# generate_text_from_prompt(prompt=d.loc[8,'text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ZSP - Data Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Starting task:  {'name': 'news', 'rows': 68, 'print_count': 10}\n",
      "2. Loaded Data, Rows:  136\n",
      "3. Prog: \n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 \n",
      "4. Saving df\n",
      "5. Ending task:  news\n",
      "\n",
      "1. Starting task:  {'name': 'spam', 'rows': 602, 'print_count': 100}\n",
      "2. Loaded Data, Rows:  1197\n",
      "3. Prog: \n",
      "0 100 200 300 400 500 600 700 800 900 1000 1100 \n",
      "4. Saving df\n",
      "5. Ending task:  spam\n",
      "\n",
      "1. Starting task:  {'name': 'sentiment', 'rows': 2499, 'print_count': 250}\n",
      "2. Loaded Data, Rows:  5000\n",
      "3. Prog: \n",
      "0 250 500 750 1000 1250 1500 1750 2000 2250 2500 2750 3000 3250 3500 3750 4000 4250 4500 4750 \n",
      "4. Saving df\n",
      "5. Ending task:  sentiment\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_main = pd.DataFrame()\n",
    "for task in zsl_tasks:\n",
    "    print(\"1. Starting task: \", task)    \n",
    "    df = load_data(task['name'])\n",
    "    print(\"2. Loaded Data, Rows: \", df.shape[0])\n",
    "    print(\"3. Prog: \")\n",
    "    for _ in range(df.shape[0]):\n",
    "        new_text = generate_text_from_prompt(prompt=df.loc[_,'text'])\n",
    "        df.loc[_,'text'] = new_text\n",
    "        df_main = df\n",
    "        if _%task['print_count'] == 0:\n",
    "            print(_, end=' ')\n",
    "            df.to_csv(\"../data/syn/mid/auto-\" + task['name'] + \"-pp-data.csv\", index=False)\n",
    "    print(\"\\n4. Saving df\")\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    df_main = df\n",
    "    df.to_csv(\"../data/syn/gpt2/pp/auto-\" + task['name'] + \"-data.csv\", index=False)\n",
    "    print(\"5. Ending task: \", task['name'])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
